{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-11T12:59:47.736981Z",
     "end_time": "2023-04-11T12:59:47.738483Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_size = 56        # 24, 56\n",
    "plots_dir = os.path.join('out', 'plots', f'n{sample_size}', 'ppf_heatmap')\n",
    "calculations_dir = os.path.join('out', 'calculations', f'n{sample_size}')\n",
    "\n",
    "\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "os.makedirs(calculations_dir, exist_ok=True)\n",
    "\n",
    "diff_metrics = {    # { file: metric name }\n",
    "    'acc_equality_diff.bin': 'Accuracy equality difference',\n",
    "    'equal_opp_diff.bin': 'Equal opportunity difference',\n",
    "    'neg_pred_parity_diff.bin': 'Negative predictive parity difference',\n",
    "    'pos_pred_parity_diff.bin': 'Positive predictive parity difference',\n",
    "    'pred_equality_diff.bin': 'Predictive equality difference',\n",
    "    'stat_parity.bin': 'Statistical parity'\n",
    "}\n",
    "ratio_metrics = {\n",
    "    'acc_equality_ratio.bin': 'Accuracy equality ratio',\n",
    "    'disp_impact.bin': 'Disparate impact',\n",
    "    'equal_opp_ratio.bin': 'Equal opportunity ratio',\n",
    "    'neg_pred_parity_ratio.bin': 'Negative predictive parity ratio',\n",
    "    'pos_pred_parity_ratio.bin': 'Positive predictive parity ratio',\n",
    "    'pred_equality_ratio.bin': 'Predictive equality ratio',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T12:59:47.740350Z",
     "end_time": "2023-04-11T12:59:47.741925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_4diff(df, metric, epsilon=0):\n",
    "    metric_f, metric_n = metric\n",
    "    print(f'Calculating {metric_n} for ε={epsilon}', end='')\n",
    "    if epsilon == 0:\n",
    "        compute_diff_prob = lambda df: np.sum(df['diff'] == 0) / len(df)\n",
    "    else:\n",
    "        compute_diff_prob = lambda df: np.sum(np.abs(df['diff']) < epsilon) / len(df)\n",
    "\n",
    "    with open(path.join(calculations_dir, f'{metric_f}'), 'rb') as f:\n",
    "        diff_metric = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['diff'])\n",
    "    print('.', end='')\n",
    "    df = pd.concat([df, diff_metric], axis=1)\n",
    "    print('.', end='')\n",
    "\n",
    "    # group by GR and IR and count probabilities\n",
    "    diff = df.groupby(['gr', 'ir']).apply(compute_diff_prob).reset_index()\n",
    "    print('.', end='')\n",
    "\n",
    "    # save to file\n",
    "    diff.to_csv(path.join(calculations_dir, f'ppf_2d_{metric_n}_e{epsilon}.csv'), index=False)\n",
    "    print('.', end='')\n",
    "\n",
    "    # clean up df\n",
    "    df.drop(columns=['diff'], inplace=True)\n",
    "    del diff_metric\n",
    "    gc.collect()\n",
    "    print(' done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T08:44:06.599923Z",
     "end_time": "2023-04-07T08:44:06.600185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load GR and IR files\n",
    "with open(path.join(calculations_dir, 'gr.bin'), 'rb') as f:\n",
    "    df = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['gr'])\n",
    "with open(path.join(calculations_dir, 'ir.bin'), 'rb') as f:\n",
    "    df = pd.concat([df,\n",
    "               pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['ir'])],\n",
    "              axis=1)\n",
    "print('Loaded GR and IR')\n",
    "\n",
    "for metric in diff_metrics.items():\n",
    "    for epsilon in [0, .01, .02]:\n",
    "        calculate_4diff(df, metric, epsilon)\n",
    "        gc.collect()\n",
    "\n",
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T00:49:17.128492Z",
     "end_time": "2023-04-07T00:59:34.223517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_4ratio(df, metric, epsilon=0):\n",
    "    metric_f, metric_n = metric\n",
    "    print(f'Calculating {metric_n} for ε={epsilon}', end='')\n",
    "    if epsilon == 0:\n",
    "        compute_prob = lambda df: np.sum(df['r'] == 1) / len(df)\n",
    "    else:\n",
    "        compute_prob = lambda df: np.sum(np.abs(df['r'] - 1) < epsilon) / len(df)\n",
    "\n",
    "    with open(path.join(calculations_dir, f'{metric_f}'), 'rb') as f:\n",
    "        diff_metric = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['r'])\n",
    "    print('.', end='')\n",
    "    df = pd.concat([df, diff_metric], axis=1)\n",
    "    print('.', end='')\n",
    "\n",
    "    # group by GR and IR and count probabilities\n",
    "    diff = df.groupby(['gr', 'ir']).apply(compute_prob).reset_index()\n",
    "    print('.', end='')\n",
    "\n",
    "    # save to file\n",
    "    diff.to_csv(path.join(calculations_dir, f'ppf_2d_{metric_n}_e{epsilon}.csv'), index=False)\n",
    "    print('.', end='')\n",
    "\n",
    "    # clean up df\n",
    "    df.drop(columns=['r'], inplace=True)\n",
    "    del diff_metric\n",
    "    gc.collect()\n",
    "    print(' done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T08:59:52.852199Z",
     "end_time": "2023-04-07T08:59:52.852450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load GR and IR files\n",
    "with open(path.join(calculations_dir, 'gr.bin'), 'rb') as f:\n",
    "    df = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['gr'])\n",
    "with open(path.join(calculations_dir, 'ir.bin'), 'rb') as f:\n",
    "    df = pd.concat([df,\n",
    "                    pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['ir'])],\n",
    "                   axis=1)\n",
    "print('Loaded GR and IR')\n",
    "\n",
    "for metric in ratio_metrics.items():\n",
    "    for epsilon in [0, .01, .02]:\n",
    "        calculate_4ratio(df, metric, epsilon)\n",
    "        gc.collect()\n",
    "\n",
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T09:11:00.578454Z",
     "end_time": "2023-04-07T09:11:00.579311Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting heatmaps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot(metric, epsilon=0):\n",
    "    metric_f, metric_n = metric\n",
    "    # load data\n",
    "    diff = pd.read_csv(path.join(calculations_dir, f'ppf_2d_{metric_n}_e{epsilon}.csv'))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pvt = diff.pivot('gr', 'ir', '0')\n",
    "    sns.heatmap(\n",
    "        pvt,\n",
    "        cmap='viridis',\n",
    "        vmin=0, vmax=.2,\n",
    "        square=True,\n",
    "        xticklabels=[f'{x:.4f}' for x in pvt.columns],\n",
    "        yticklabels=[f'{y:.4f}' for y in pvt.index],\n",
    "    )\n",
    "\n",
    "    # labels and layout\n",
    "    plt.title(f'{metric_n}: probability of {\"im\" if epsilon != 0 else \"\"}perfect fairness (ε={epsilon})')\n",
    "    plt.xlabel('IR')\n",
    "    plt.ylabel('GR')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # invert y\n",
    "    ax = plt.gca()\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.savefig(path.join(plots_dir, f'aaa_heatmap_{metric_n}_e{epsilon}.png'))\n",
    "    return plt.gcf()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T13:13:03.239303Z",
     "end_time": "2023-04-11T13:13:03.279947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric in diff_metrics.items():\n",
    "    for epsilon in [0, .01, .02]:\n",
    "        fig = plot(metric, epsilon)\n",
    "        # plt.show()\n",
    "        plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T13:13:04.832320Z",
     "end_time": "2023-04-11T13:13:11.890272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric in ratio_metrics.items():\n",
    "    for epsilon in [0, .01, .02]:\n",
    "        fig = plot(metric, epsilon)\n",
    "        # plt.show()\n",
    "        plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T09:13:03.884206Z",
     "end_time": "2023-04-07T09:13:03.884474Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd72be3cedac9db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Scatter plot fairness vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:42:52.823449Z",
     "start_time": "2024-02-07T14:42:52.781168Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "from os import path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 56\n",
    "calculations_dir = path.join('out', 'calculations', f'n{sample_size}')\n",
    "plots_dir = path.join('out', 'plots', f'n{sample_size}', 'scatter')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "metrics = {\n",
    "    'acc_equality_diff.bin': 'Accuracy equality',\n",
    "    'equal_opp_diff.bin': 'Equal opportunity',\n",
    "    'pred_equality_diff.bin': 'Predictive equality',\n",
    "    'stat_parity.bin': 'Statistical parity',\n",
    "    'neg_pred_parity_diff.bin': 'Negative predictive parity',\n",
    "    'pos_pred_parity_diff.bin': 'Positive predictive parity',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73c2160b057102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:42:53.899629Z",
     "start_time": "2024-02-07T14:42:53.892457Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    data_cols = [\n",
    "        'i_tp',     # minority true positive\n",
    "        'i_fp',     # minority false positive\n",
    "        'i_tn',     # minority true negative\n",
    "        'i_fn',     # minority false negative\n",
    "        'j_tp',     # majority true positive\n",
    "        'j_fp',     # majority false positive\n",
    "        'j_tn',     # majority true negative\n",
    "        'j_fn',     # majority false negative\n",
    "    ]\n",
    "    sample_size = 56\n",
    "    dataset_path = path.join('out', f'Set(08,{sample_size}).bin')\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        df = pd.DataFrame(pickle.load(f), columns=data_cols)\n",
    "\n",
    "    acc = (df['i_tp'] + df['i_tn'] + df['j_tp'] + df['j_tn']) / sample_size\n",
    "\n",
    "    del df\n",
    "    return acc\n",
    "\n",
    "def get_gmean():\n",
    "    data_cols = [\n",
    "        'i_tp',     # minority true positive\n",
    "        'i_fp',     # minority false positive\n",
    "        'i_tn',     # minority true negative\n",
    "        'i_fn',     # minority false negative\n",
    "        'j_tp',     # majority true positive\n",
    "        'j_fp',     # majority false positive\n",
    "        'j_tn',     # majority true negative\n",
    "        'j_fn',     # majority false negative\n",
    "    ]\n",
    "    sample_size = 56\n",
    "    dataset_path = path.join('out', f'Set(08,{sample_size}).bin')\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        df = pd.DataFrame(pickle.load(f), columns=data_cols)\n",
    "\n",
    "    gm = ((df['i_tp'] + df['j_tp']) * (df['i_tn'] + df['j_tn']) / (df['i_tp'] + df['j_tp'] + df['i_fn'] + df['j_fn']) / (df['i_tn'] + df['j_tn'] + df['i_fp'] + df['j_fp'])).pow(1/2)\n",
    "\n",
    "    del df\n",
    "    return gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff4886f29edac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T13:12:04.290944Z",
     "start_time": "2024-02-07T13:11:52.937477Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # uncomment if accuracy is not calculated yet\n",
    "\n",
    "# with open(path.join(calculations_dir, \"accuracy.bin\"), \"wb+\") as f:\n",
    "#     get_accuracy().to_numpy().tofile(f)\n",
    "\n",
    "# with open(path.join(calculations_dir, \"g_mean.bin\"), \"wb+\") as f:\n",
    "#     get_gmean().to_numpy().tofile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22b4bcfb457908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:53:13.721618Z",
     "start_time": "2024-02-07T14:53:13.715814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_counts(accuracy, metric_file, metric_name, acc_name='accuracy'):\n",
    "    with open(path.join(calculations_dir, metric_file), 'rb') as f:\n",
    "        df = pd.concat([\n",
    "            accuracy,\n",
    "            pd.DataFrame(np.fromfile(f).astype(np.float16), columns=[metric_name])\n",
    "        ], axis=1)\n",
    "\n",
    "    df = df.groupby([acc_name, metric_name]).size().reset_index(name='count')\n",
    "\n",
    "    fn = path.join(calculations_dir, f'counts_{acc_name}_vs_{metric_file.replace(\".bin\", \".csv\")}')\n",
    "    with open(fn, 'w+') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print(fn)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ed0127eba8b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:53:13.993610Z",
     "start_time": "2024-02-07T14:53:13.989169Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scatter with point size\n",
    "\n",
    "def scatter(metric_file, metric_name, acc_name='accuracy'):\n",
    "    with open(path.join(calculations_dir, f'counts_{acc_name}_vs_{metric_file.replace(\".bin\", \".csv\")}'), 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 8))\n",
    "    ax.scatter(\n",
    "        df[acc_name],\n",
    "        df[metric_name],\n",
    "        # s=np.log2(df['count']),\n",
    "        s=np.log2(df['count'] / 10),\n",
    "        alpha=.1,\n",
    "        lw=0,\n",
    "    )\n",
    "    ax.set_xlabel(acc_name)\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title('v2')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path.join(plots_dir, f'scatter_{acc_name}_vs_{metric_name}.png'), dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21807524d914bcc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:54:07.034662Z",
     "start_time": "2024-02-07T14:54:07.026526Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def heatmap(metric_file, metric_name, acc_name='accuracy'):\n",
    "    # grouped by rounding\n",
    "\n",
    "    with open(os.path.join(calculations_dir,\n",
    "                        f'counts_{acc_name}_vs_{metric_file.replace(\".bin\", \".csv\")}'),\n",
    "              'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    df[acc_name] = df[acc_name].round(2)\n",
    "    df[metric_name] = df[metric_name].round(2)\n",
    "    df = df.groupby([acc_name, metric_name]).sum().reset_index().pivot(index=acc_name, columns=metric_name, values='count')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 8))\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=False,\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        cmap='cividis',\n",
    "        # alpha=.1,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(metric_name)\n",
    "    ax.set_ylabel(acc_name)\n",
    "\n",
    "    ax.set_title(f'{metric_name} vs {acc_name}')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(plots_dir, f'hm_v2_{acc_name}_vs_{metric_name}.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edfe295dfbb21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:54:09.956108Z",
     "start_time": "2024-02-07T14:54:07.225321Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # read the metric values from previously saved files\n",
    "\n",
    "with open(path.join(calculations_dir, 'accuracy.bin'), 'rb') as f:\n",
    "    acc = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['accuracy'])\n",
    "\n",
    "# with open(path.join(calculations_dir, 'g_mean.bin'), 'rb') as f:\n",
    "#     g_mean = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['g_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ae0b1711f91a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:54:16.746146Z",
     "start_time": "2024-02-07T14:54:09.957438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_f = scatter\n",
    "plot_f = heatmap\n",
    "\n",
    "for mf, mn in metrics.items():\n",
    "    # save_counts(g_mean, mf, mn, 'g_mean')\n",
    "    plot_f(mf, mn, 'acc')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dc3a6fec25ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T14:43:37.080761Z",
     "start_time": "2024-02-07T14:43:08.315288Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# g_mean vs accuracy\n",
    "save_counts(acc, 'g_mean.bin', 'g_mean', 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

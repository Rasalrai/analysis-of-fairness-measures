{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Perfect Fairness and undefined values\n",
    "\n",
    "calculations for different metrics, group ratios and imbalance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 56\n",
    "# setting epsilon to another (small positive) value allows to calculate the probability of being epsilon-close to perfect fairness\n",
    "epsilon = 0\n",
    "\n",
    "calculations_dir = path.join('out', 'calculations', f'n{sample_size}')\n",
    "os.makedirs(calculations_dir, exist_ok=True)\n",
    "dataset_path = path.join('..', 'fairness-data-generator', 'out', f'Set(08,{sample_size}).bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate values for visualizations\n",
    "diff_metrics = {  # { file: metric name }\n",
    "    'pos_pred_parity_diff.bin': 'Positive predictive parity difference',\n",
    "    'acc_equality_diff.bin': 'Accuracy equality difference',\n",
    "    'stat_parity.bin': 'Statistical parity difference',\n",
    "    'equal_opp_diff.bin': 'Equal opportunity difference',\n",
    "    'neg_pred_parity_diff.bin': 'Negative predictive parity difference',\n",
    "    'pred_equality_diff.bin': 'Predictive equality difference',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ppf_diff(df, metrics, ratio_type, epsilon=0):\n",
    "    pf_probs, nan_probs = {}, {}\n",
    "\n",
    "    if epsilon == 0:\n",
    "        compute_diff_prob = lambda df: np.sum(df['diff'] == 0) / len(df)\n",
    "    else:\n",
    "        compute_diff_prob = lambda df: np.sum(np.abs(df['diff']) < epsilon) / len(df)\n",
    "\n",
    "    for metric_file, metric_name in metrics.items():\n",
    "        print(metric_name)\n",
    "        with open(path.join(calculations_dir, metric_file), 'rb') as f:\n",
    "            diff_metric = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['diff'])\n",
    "        df = pd.concat([df, diff_metric], axis=1)\n",
    "\n",
    "        pf_bygroup = list()\n",
    "        nans_bygroup = list()\n",
    "\n",
    "        for gn, group in df.groupby(ratio_type):\n",
    "            if group['diff'].isna().all():\n",
    "                pf_bygroup.append([gn, np.nan])\n",
    "            else:\n",
    "                pf_bygroup.append([gn, compute_diff_prob(group)])\n",
    "            nans_bygroup.append([gn, group['diff'].isna().sum() / group.shape[0]])\n",
    "\n",
    "        pf_bygroup = pd.DataFrame(pf_bygroup, columns=[ratio_type, 'diff'])\n",
    "        pf_probs[metric_name] = pf_bygroup['diff']\n",
    "\n",
    "        nans_bygroup = pd.DataFrame(nans_bygroup, columns=[ratio_type, 'diff'])\n",
    "        nan_probs[metric_name] = nans_bygroup['diff']\n",
    "\n",
    "        # the dataframe (first col) can be reused for the next metric\n",
    "        df.drop('diff', axis=1, inplace=True)\n",
    "\n",
    "    pf_probs[ratio_type] = pf_bygroup[ratio_type]\n",
    "    pf_df = pd.DataFrame(pf_probs).reset_index()\n",
    "    pf_df.to_csv(path.join(calculations_dir, f'perfect_fairness_{ratio_type}_eps{epsilon}.csv'), index=False)\n",
    "\n",
    "    nan_probs[ratio_type] = nans_bygroup[ratio_type]\n",
    "    nan_df = pd.DataFrame(nan_probs).reset_index()\n",
    "    nan_df.to_csv(path.join(calculations_dir, f'nans_{ratio_type}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ratio in ['ir', 'gr']:\n",
    "    with open(path.join(calculations_dir, f'{ratio}.bin'), 'rb') as f:\n",
    "        df = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=[ratio])\n",
    "    calculate_ppf_diff(df, diff_metrics, ratio, epsilon)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "reading data from the csv files created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 56        # 24\n",
    "calculations_dir = path.join('out', 'calculations', f'n{sample_size}')\n",
    "plots_dir = path.join('out', 'plots', f'n{sample_size}', 'perfect_fairness')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "epsilons = [0,]\n",
    "ratio_types = ['gr', 'ir']\n",
    "\n",
    "\n",
    "dfs = {(ratio_type, epsilon): pd.read_csv(path.join(calculations_dir, f'perfect_fairness_{ratio_type}_eps{epsilon}.csv'))\n",
    "       for ratio_type in ratio_types\n",
    "       for epsilon in epsilons}\n",
    "\n",
    "# colour scheme inspired by https://personal.sron.nl/~pault/\n",
    "diff_metrics_styles = {\n",
    "    'Accuracy equality difference': {'color': '#6699CC', 'marker': '*'},\n",
    "    'Statistical parity difference': {'color': '#994455', 'marker': '.'},\n",
    "\n",
    "    'Equal opportunity difference': {'color': '#004488', 'marker': 'v'},\n",
    "    'Predictive equality difference': {'color': '#997700', 'marker': 'x'},\n",
    "\n",
    "    'Negative predictive parity difference': {'color': '#EECC66', 'marker': '+'},\n",
    "    'Positive predictive parity difference': {'color': '#EE99AA', 'marker': 'o'},\n",
    "}\n",
    "\n",
    "x_description = {\n",
    "    'gr': 'Protected group ratio (GR)',\n",
    "    'ir': 'Imbalance ratio (IR)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_df(df, base_metric):\n",
    "    temp = df.pop(base_metric)\n",
    "    length = len(df.columns)\n",
    "    df = pd.melt(df)\n",
    "    df[base_metric] = list(temp) * length\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mlp(df, base_metric, color_mapping, title='Proportion of perfect fairness', y_max=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for col in color_mapping.keys():\n",
    "        ax.plot(df[base_metric], df[col], label=col.replace('difference', ''), alpha=.5, **color_mapping[col])\n",
    "\n",
    "    if y_max is not None:\n",
    "        ax.set_ylim(0, y_max)\n",
    "\n",
    "    ax.set_xlabel(x_description[base_metric])\n",
    "    ax.set_ylabel('Probability of perfect fairness')\n",
    "    ax.set_title(title)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio_type in ratio_types:\n",
    "    for eps in epsilons:\n",
    "        fig = plot_mlp(dfs[(ratio_type, eps)], ratio_type, diff_metrics_styles,\n",
    "                       title='')\n",
    "        fig.savefig(path.join(plots_dir, f'ppf_{ratio_type}_eps{eps}.svg'),\n",
    "                    dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of NaN - plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_probability(df, base_metric, color_mapping, title='Probability of NaN', y_max=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for col in color_mapping.keys():\n",
    "        ax.plot(df[base_metric], df[col], label=col.replace('difference', ''), alpha=.5, **color_mapping[col])\n",
    "\n",
    "    if y_max is not None:\n",
    "        ax.set_ylim(0, y_max)\n",
    "\n",
    "    ax.set_xlabel(x_description[base_metric])\n",
    "    ax.set_ylabel('Probability of undefined metric value')\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_dfs = {ratio_type: pd.read_csv(path.join(calculations_dir, f'nans_{ratio_type}.csv'))\n",
    "           for ratio_type in ratio_types}\n",
    "\n",
    "for ratio_type in ratio_types:\n",
    "    fig = nan_probability(nan_dfs[ratio_type], ratio_type, diff_metrics_styles,\n",
    "                          title='')\n",
    "    fig.savefig(path.join(plots_dir, f'nan_{ratio_type}_line.svg'), dpi=300)\n",
    "\n",
    "for ratio_type in ratio_types:\n",
    "    fig = nan_probability(nan_dfs[ratio_type], ratio_type, diff_metrics_styles,\n",
    "                          title=f'Probability of NaN for given value of {ratio_type.upper()}', y_max=.02)\n",
    "    fig.savefig(path.join(plots_dir, f'nan_{ratio_type}_zoom_line.svg'), dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Dataset, aka Census income\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:05.609767452Z",
     "start_time": "2023-05-07T06:57:04.293776326Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, recall_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils import Timer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "\n",
    "# colour scheme inspired by https://personal.sron.nl/~pault/\n",
    "colours = ['#EE7733', '#33BBEE', '#EE3377', '#888888', '#009988', \"#332288\"]\n",
    "\n",
    "x_labels = {\n",
    "    'gr': 'Protected group ratio (GR)',\n",
    "    'ir': 'Imbalance ratio (IR)',\n",
    "}\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 13\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "timer_dir = path.join('out', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:18.086900469Z",
     "start_time": "2023-05-07T06:57:17.877235449Z"
    }
   },
   "outputs": [],
   "source": [
    "timer = Timer().start()\n",
    "\n",
    "features = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',       # removed\n",
    "    'education',    # sorted later on\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "]\n",
    "dataset = pd.read_csv('data/adult.data', sep=', ', na_values=['?', ' ?'],\n",
    "                      header=0, names=features + ['income'])\n",
    "dataset.drop(columns=['fnlwgt'], inplace=True)\n",
    "features.remove('fnlwgt')\n",
    "\n",
    "timer.checkpoint(f\"read data\")\n",
    "timer.reset()\n",
    "\n",
    "plots_dir = os.path.join('out', 'plots', 'case_study', 'census_income')\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data into subsets\n",
    "\n",
    "To enable comparison for different GR and IR.\n",
    "\n",
    "The split is proportional to group/class sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:27.030981884Z",
     "start_time": "2023-05-07T06:57:26.957788453Z"
    }
   },
   "outputs": [],
   "source": [
    "n = dataset.shape[0]\n",
    "gr = ir = .5\n",
    "\n",
    "# [majority, minority]\n",
    "sex = ['Male', 'Female']\n",
    "income = ['<=50K', '>50K']\n",
    "\n",
    "\n",
    "def split_data(df, n, gr, ir):\n",
    "    \"\"\"\n",
    "    :param df: original data\n",
    "    :param n: final size of the sample\n",
    "    :return: the sample\n",
    "    \"\"\"\n",
    "    # set ratios of sex and income\n",
    "    f0 = round(n * gr * (1 - ir))\n",
    "    f1 = round(n * gr * ir)\n",
    "    m0 = round(n * (1 - gr) * (1 - ir))\n",
    "    m1 = round(n * (1 - gr) * ir)\n",
    "\n",
    "    sample = pd.concat([\n",
    "        df[(df['sex'] == sex[1]) & (df['income'] == income[0])].sample(n=int(f0), random_state=2137),\n",
    "        df[(df['sex'] == sex[1]) & (df['income'] == income[1])].sample(n=int(f1), random_state=2137),\n",
    "        df[(df['sex'] == sex[0]) & (df['income'] == income[0])].sample(n=int(m0), random_state=2137),\n",
    "        df[(df['sex'] == sex[0]) & (df['income'] == income[1])].sample(n=int(m1), random_state=2137),\n",
    "    ]).reset_index(drop=True)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing and helpers for classification/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:28.990998384Z",
     "start_time": "2023-05-07T06:57:28.859690686Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_fs = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "]\n",
    "\n",
    "education_order = [\n",
    "    'Preschool',\n",
    "    '1st-4th',\n",
    "    '5th-6th',\n",
    "    '7th-8th',\n",
    "    '9th',\n",
    "    '10th',\n",
    "    '11th',\n",
    "    '12th',\n",
    "    'HS-grad',\n",
    "    'Some-college',\n",
    "    'Assoc-acdm',\n",
    "    'Assoc-voc',\n",
    "    'Bachelors',\n",
    "    'Masters',\n",
    "    'Prof-school',\n",
    "    'Doctorate',\n",
    "]\n",
    "\n",
    "# get the columns in the correct order\n",
    "cols = np.concatenate([dataset.columns.copy(deep=True).drop(categorical_fs + ['income']), categorical_fs])\n",
    "cols_d = {c: i for i, c in enumerate(cols)}\n",
    "\n",
    "classifiers = [\n",
    "    [RandomForestClassifier, {'random_state': 2137}],\n",
    "    [DecisionTreeClassifier, {'random_state': 2137}],\n",
    "    [GaussianNB, {}],\n",
    "    [LogisticRegression, {}],\n",
    "    [KNeighborsClassifier, {}],\n",
    "    [MLPClassifier, {'random_state': 2137}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:29.458700514Z",
     "start_time": "2023-05-07T06:57:29.370643576Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    X_all = dataset[features]\n",
    "    y_all = LabelEncoder().fit_transform(dataset['income'])\n",
    "\n",
    "    # encode categorical features\n",
    "    data_encoder = OrdinalEncoder().fit(X_all[categorical_fs])\n",
    "    X_categorical = data_encoder.transform(X_all[categorical_fs])\n",
    "\n",
    "    edu_encoder = OrdinalEncoder(categories=[education_order]).fit(X_all[['education']])\n",
    "    X_categorical[:, categorical_fs.index('education')] = edu_encoder.transform(X_all[['education']])[0]\n",
    "\n",
    "    # finally, the features\n",
    "    X_all = np.concatenate([X_all.drop(categorical_fs, axis=1), X_categorical], axis=1)\n",
    "\n",
    "    return X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:31.835148625Z",
     "start_time": "2023-05-07T06:57:31.832033454Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_fairness(clf, X, y, protected, group=1, cls=1):\n",
    "    \"\"\"\n",
    "    :param protected: id/name of the protected attribute column\n",
    "    :param group: id of the protected group\n",
    "    :param cls: id of the positive class\n",
    "    :return: dictionary of fairness metrics for the given classifier's results\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict(X)\n",
    "    # columns: protected_value, y_true, y_pred\n",
    "    labelled = np.concatenate([\n",
    "        X[:, protected].reshape(-1, 1),\n",
    "        y.reshape(-1, 1),\n",
    "        y_pred.reshape(-1, 1)\n",
    "    ], axis=1)\n",
    "\n",
    "    # calculate confusion matrices\n",
    "    cms = [None, None]\n",
    "\n",
    "    # y true/pred for the protected group\n",
    "    ys = labelled[labelled[:, 0] == group]\n",
    "    cms[0] = confusion_matrix(ys[:, 1], ys[:, 2], labels=[0, 1])\n",
    "    # ... and for the other (unprotected) group\n",
    "    ys = labelled[labelled[:, 0] != group]\n",
    "    cms[1] = confusion_matrix(ys[:, 1], ys[:, 2], labels=[0, 1])\n",
    "\n",
    "    # mj = majority - unprotected\n",
    "    # mr = minority - protected\n",
    "    mr, mj = group, 1 - group\n",
    "    pos, neg = cls, 1 - cls\n",
    "\n",
    "    # labels for the confusion matrix items\n",
    "    tn = (neg, neg)\n",
    "    fp = (neg, pos)\n",
    "    fn = (pos, neg)\n",
    "    tp = (pos, pos)\n",
    "\n",
    "    # calculate fairness metrics\n",
    "    fairness = dict()\n",
    "\n",
    "    # Accuracy Equality Difference\n",
    "    fairness['Accuracy Equality Difference'] = \\\n",
    "        (cms[mj].item(tp) + cms[mj].item(tn)) / cms[mj].sum() - \\\n",
    "        (cms[mr].item(tp) + cms[mr].item(tn)) / cms[mr].sum()\n",
    "\n",
    "    # Equal Opportunity Difference: j_tpr - i_tpr\n",
    "    try:\n",
    "        fairness['Equal Opportunity Difference'] = \\\n",
    "            cms[mj].item(tp) / (cms[mj].item(tp) + cms[mj].item(fn)) - \\\n",
    "            cms[mr].item(tp) / (cms[mr].item(tp) + cms[mr].item(fn))\n",
    "    except ZeroDivisionError:\n",
    "        fairness['Equal Opportunity Difference'] = np.nan\n",
    "\n",
    "    # Predictive Equality Difference: j_fpr - i_fpr\n",
    "    try:\n",
    "        fairness['Predictive Equality Difference'] = \\\n",
    "            cms[mj].item(fp) / (cms[mj].item(fp) + cms[mj].item(tn)) - \\\n",
    "            cms[mr].item(fp) / (cms[mr].item(fp) + cms[mr].item(tn))\n",
    "    except ZeroDivisionError:\n",
    "        fairness['Predictive Equality Difference'] = np.nan\n",
    "\n",
    "    # Positive Predictive Parity Difference: j_ppv - i_ppv\n",
    "    try:\n",
    "        fairness['Positive Predictive Parity Difference'] = \\\n",
    "            cms[mj].item(tp) / (cms[mj].item(tp) + cms[mj].item(fp)) - \\\n",
    "            cms[mr].item(tp) / (cms[mr].item(tp) + cms[mr].item(fp))\n",
    "    except ZeroDivisionError:\n",
    "        fairness['Positive Predictive Parity Difference'] = np.nan\n",
    "\n",
    "    # Negative Predictive Parity Difference: j_npv - i_npv\n",
    "    try:\n",
    "        fairness['Negative Predictive Parity Difference'] = \\\n",
    "            cms[mj].item(tn) / (cms[mj].item(tn) + cms[mj].item(fn)) - \\\n",
    "            cms[mr].item(tn) / (cms[mr].item(tn) + cms[mr].item(fn))\n",
    "    except ZeroDivisionError:\n",
    "        fairness['Negative Predictive Parity Difference'] = np.nan\n",
    "\n",
    "    fairness['Statistical Parity Difference'] = \\\n",
    "        (cms[mj].item(tp) + cms[mj].item(fp)) / cms[mj].sum() - \\\n",
    "        (cms[mr].item(tp) + cms[mr].item(fp)) / cms[mr].sum()\n",
    "\n",
    "    return fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:33.161897004Z",
     "start_time": "2023-05-07T06:57:33.083771079Z"
    }
   },
   "outputs": [],
   "source": [
    "# group by metric\n",
    "def plot_fairness_gb_metric(fairness, gr, ir):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    ax.set_title(f'Fairness metrics for different classifiers; GR = {gr}, IR = {ir}')\n",
    "    ax.set_ylabel('Fairness metric value')\n",
    "\n",
    "    metrics = ['\\n'.join([' '.join(f.split(\" \")[:2]), ' '.join(f.split(\" \")[2:])])\n",
    "               for f in fairness[list(fairness.keys())[0]].keys()]\n",
    "    xticks = np.arange(len(metrics))\n",
    "    width = 1. / (len(fairness.keys()) + 2)\n",
    "\n",
    "    for i, (clf, f) in enumerate(fairness.items()):\n",
    "        ax.bar(xticks + i * width, f.values(), width, label=clf.replace('Classifier', ''), color=colours[i])\n",
    "\n",
    "    ax.set_xticks(xticks + width * len(fairness.keys()) / 2, metrics, rotation=45)\n",
    "    ax.legend(ncols=1)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_fairness_gb_clf(fairness, gr, ir):\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    ax.set_title(f'Fairness metrics for different classifiers; GR = {gr}, IR = {ir}')\n",
    "    ax.set_ylabel('Fairness metric value')\n",
    "\n",
    "    metrics = fairness[list(fairness.keys())[0]].keys()\n",
    "    classifiers = [c.replace('Classifier', '') for c in fairness.keys()]\n",
    "    xticks = np.arange(len(classifiers))\n",
    "    width = 1. / (len(metrics) + 2)\n",
    "    shift = np.arange(len(metrics)) * width\n",
    "\n",
    "    for i, (clf, f) in enumerate(fairness.items()):\n",
    "        ax.bar(i + shift, f.values(), width, color=colours[:len(metrics)])\n",
    "    ax.set_xticks(xticks + width * len(metrics) / 2, classifiers)\n",
    "    ax.legend(handles=[mpatches.Patch(color=c, label=m) for c, m in zip(colours, metrics)], ncol=1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T06:57:35.152527289Z",
     "start_time": "2023-05-07T06:57:34.995700999Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "holdout_splits = 50\n",
    "holdout = ShuffleSplit(n_splits=holdout_splits, test_size=.33, random_state=2137)\n",
    "SAMPLE_SIZE = 1100\n",
    "\n",
    "rs = [.01, .02, .05] + [round(x, 2) for x in np.arange(.1, 1., .1)] + [.95, .98, .99]\n",
    "ratios = [[.5, ir] for ir in rs] + [[gr, .5] for gr in rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:16.985595Z",
     "start_time": "2024-02-05T22:34:48.674074Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculations\n",
    "fairness_results = []\n",
    "results = []\n",
    "timer.start()\n",
    "\n",
    "for gr, ir in ratios:\n",
    "    print(f'GR: {gr}, IR: {ir}')\n",
    "    swap_gr, swap_ir = False, False\n",
    "\n",
    "    df = split_data(dataset, SAMPLE_SIZE, gr, ir)\n",
    "    X_all, y_all = preprocess(df)\n",
    "    timer.checkpoint(f\"gr={gr} ir={ir} data preprocessing\")\n",
    "\n",
    "    for i, (traini, testi) in enumerate(holdout.split(X_all)):\n",
    "        X_train, X_test = X_all[traini], X_all[testi]\n",
    "        y_train, y_test = y_all[traini], y_all[testi]\n",
    "\n",
    "        for clf, kwargs in classifiers:\n",
    "            pipe = make_pipeline(\n",
    "                KNNImputer(),\n",
    "                StandardScaler(),\n",
    "                clf(**kwargs)\n",
    "            ).fit(X_train, y_train)\n",
    "            f = calculate_fairness(pipe, X_test, y_test, cols_d['sex'], group=1-int(swap_gr), cls=1-int(swap_ir))\n",
    "\n",
    "            for p_metric in [roc_auc_score, geometric_mean_score, recall_score, f1_score]:\n",
    "                results.append([gr, ir, clf.__name__.replace('Classifier', ''), p_metric.__name__, p_metric(y_test, pipe.predict(X_test), labels=[0, 1])])\n",
    "\n",
    "            for metric, value in f.items():\n",
    "                fairness_results.append([gr, ir, clf.__name__.replace('Classifier', ''), metric, value])\n",
    "            timer.checkpoint(f\"gr={gr} ir={ir} classification with {clf.__name__} rep. {i}\")\n",
    "\n",
    "results_cv = pd.DataFrame(results, columns=['gr', 'ir', 'clf', 'metric', 'value'])\n",
    "fairness_results_cv = pd.DataFrame(fairness_results, columns=['gr', 'ir', 'clf', 'metric', 'value'])\n",
    "\n",
    "timer.checkpoint(f\"saving results\")\n",
    "timer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:17.017805Z",
     "start_time": "2024-02-05T22:51:16.663719Z"
    }
   },
   "outputs": [],
   "source": [
    "# # pickle the results\n",
    "\n",
    "with open(os.path.join('out', 'fairness_results_cv.pkl'), 'wb') as f:\n",
    "    pickle.dump(fairness_results_cv, f)\n",
    "\n",
    "with open(os.path.join('out', 'clf_results_cv.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:28:55.349658Z",
     "start_time": "2024-02-07T00:28:55.334140Z"
    }
   },
   "outputs": [],
   "source": [
    "# unpickle - for reusing the results\n",
    "\n",
    "with open(os.path.join('out', 'fairness_results_cv.pkl'), 'rb') as f:\n",
    "    fairness_results_cv = pickle.load(f)\n",
    "\n",
    "with open(os.path.join('out', 'clf_results_cv.pkl'), 'rb') as f:\n",
    "    results_cv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:17.386041Z",
     "start_time": "2024-02-05T22:51:16.664913Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # save to csv\n",
    "# fairness_results_cv.to_csv(os.path.join('out', 'fairness_results_cv.csv'), index=False)\n",
    "# results_cv.to_csv(os.path.join('out', 'clf_results_cv.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line graph: `fairness(ratio)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T20:27:23.969544262Z",
     "start_time": "2023-05-04T20:27:06.163936301Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_line(fairness: pd.DataFrame, metric: str, ratio_type: str, fill='std', ylim=(-.5, .5)):\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel(ratio_type.upper())\n",
    "\n",
    "    metrics = fairness['metric'].unique()\n",
    "    clfs = fairness['clf'].unique()\n",
    "    ratios = sorted(fairness[ratio_type].unique())\n",
    "    other_ratio = 'gr' if ratio_type == 'ir' else 'ir'\n",
    "    mean, stdev, err = {}, {}, {}\n",
    "\n",
    "    for r in ratios:\n",
    "        for clf in clfs:\n",
    "            subset = fairness[\n",
    "                (fairness[ratio_type] == r) &\n",
    "                (fairness['clf'] == clf) &\n",
    "                (fairness[other_ratio] == .5) &\n",
    "                (fairness['metric'] == metric) &\n",
    "                fairness['value'].notna()\n",
    "            ]\n",
    "            mean[(r, clf)] = subset['value'].mean(skipna=True)\n",
    "            stdev[(r, clf)] = subset['value'].std(skipna=True)\n",
    "            err[(r, clf)] = scipy.stats.sem(subset['value'], nan_policy='omit')\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', alpha=.3)\n",
    "\n",
    "    for i, clf in enumerate(clfs):\n",
    "        ax.plot(ratios, [mean[(r, clf)] for r in ratios], label=clf, color=colours[i], marker='o')\n",
    "        if fill == 'err':\n",
    "            ax.fill_between(ratios,\n",
    "                            [mean[(r, clf)] - err[(r, clf)] for r in ratios],\n",
    "                            [mean[(r, clf)] + err[(r, clf)] for r in ratios],\n",
    "                            alpha=.15, color=colours[i])\n",
    "        elif fill == 'std':\n",
    "            ax.fill_between(ratios,\n",
    "                            [mean[(r, clf)] - stdev[(r, clf)] for r in ratios],\n",
    "                            [mean[(r, clf)] + stdev[(r, clf)] for r in ratios],\n",
    "                            alpha=.15, color=colours[i])\n",
    "\n",
    "    ax.legend(loc=9)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "    # workaround to keep the x tick labels readable\n",
    "    ratios_ticks = ['0.01', '  \\n0.02',\n",
    "                    '0.05', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '0.95',\n",
    "                    '0.98\\n  ', '0.99']\n",
    "\n",
    "    ax.set_xticks(ratios, ratios_ticks, rotation=90)\n",
    "    ax.set_xlim(0, 1)\n",
    "    if ylim:\n",
    "        ax.set_ylim(*ylim)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:29:16.610478Z",
     "start_time": "2024-02-07T00:29:04.480151Z"
    }
   },
   "outputs": [],
   "source": [
    "for fill in ('std', 'err'):\n",
    "    subdir = f'line_{fill}'\n",
    "    os.makedirs(os.path.join(plots_dir, subdir), exist_ok=True)\n",
    "\n",
    "    for ratio_type, ylim in [\n",
    "        ('ir', (-.9, .9)),\n",
    "        ('gr', (-.9, .9)),\n",
    "        ]:\n",
    "        for metric in fairness_results_cv['metric'].unique():\n",
    "            fig = plot_line(fairness_results_cv, metric, ratio_type, ylim=ylim, fill=fill)\n",
    "            fig.savefig(os.path.join(plots_dir, subdir, f'fairness_line_{ratio_type}_{metric}.png'))\n",
    "            fig.savefig(os.path.join(plots_dir, subdir, f'fairness_line_{ratio_type}_{metric}.pdf'))\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the absolute value of fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:29:16.619190Z",
     "start_time": "2024-02-07T00:29:16.615332Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_line_abs(fairness: pd.DataFrame, metric: str, ratio_type: str, fill='std', ylim=None):\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel(ratio_type.upper())\n",
    "\n",
    "    metrics = fairness['metric'].unique()\n",
    "    clfs = fairness['clf'].unique()\n",
    "    ratios = sorted(fairness[ratio_type].unique())\n",
    "    other_ratio = 'gr' if ratio_type == 'ir' else 'ir'\n",
    "    mean, stdev, err = {}, {}, {}\n",
    "\n",
    "    for r in ratios:\n",
    "        for clf in clfs:\n",
    "            subset = fairness[\n",
    "                (fairness[ratio_type] == r) &\n",
    "                (fairness['clf'] == clf) &\n",
    "                (fairness[other_ratio] == .5) &\n",
    "                (fairness['metric'] == metric) &\n",
    "                fairness['value'].notna()\n",
    "                ]\n",
    "            mean[(r, clf)] = subset['value'].abs().mean(skipna=True)\n",
    "            stdev[(r, clf)] = subset['value'].abs().std(skipna=True)\n",
    "            err[(r, clf)] = scipy.stats.sem(subset['value'].abs(), nan_policy='omit')\n",
    "\n",
    "    for i, clf in enumerate(clfs):\n",
    "        ax.plot(ratios, [mean[(r, clf)] for r in ratios], label=clf, color=colours[i], marker='o')\n",
    "        if fill == 'err':\n",
    "            ax.fill_between(ratios,\n",
    "                            [mean[(r, clf)] - err[(r, clf)] for r in ratios],\n",
    "                            [mean[(r, clf)] + err[(r, clf)] for r in ratios],\n",
    "                            alpha=.15, color=colours[i])\n",
    "        elif fill == 'std':\n",
    "            ax.fill_between(ratios,\n",
    "                            [mean[(r, clf)] - stdev[(r, clf)] for r in ratios],\n",
    "                            [mean[(r, clf)] + stdev[(r, clf)] for r in ratios],\n",
    "                            alpha=.15, color=colours[i])\n",
    "\n",
    "    ax.legend(loc=9)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(ratios, ratios, rotation=90)\n",
    "    ax.set_xlim(0, 1)\n",
    "    if ylim:\n",
    "        ax.set_ylim(*ylim)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:29:28.206154Z",
     "start_time": "2024-02-07T00:29:16.618967Z"
    }
   },
   "outputs": [],
   "source": [
    "for fill in ('std', 'err'):\n",
    "    subdir = f'line_abs_{fill}'\n",
    "    os.makedirs(os.path.join(plots_dir, subdir), exist_ok=True)\n",
    "\n",
    "    for ratio_type in ['ir', 'gr']:\n",
    "        for metric in fairness_results_cv['metric'].unique():\n",
    "            fig = plot_line_abs(fairness_results_cv, metric, ratio_type, ylim=(0, .6), fill=fill)\n",
    "            fig.savefig(os.path.join(plots_dir, subdir, f'fairness_line_{ratio_type}_{metric}_{fill}_abs_rh.png'))\n",
    "            fig.savefig(os.path.join(plots_dir, subdir, f'fairness_line_{ratio_type}_{metric}_{fill}_abs_rh.pdf'))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot nan count\n",
    "\n",
    "check how many results are undefined for the metrics and ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T14:54:38.163777Z",
     "start_time": "2023-05-03T14:54:37.276486Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_nan(fairness, ratio_type, clfs=None, metrics=None, ylim=None):\n",
    "    if clfs is None:\n",
    "        clfs = fairness['clf'].unique()\n",
    "    if metrics is None:\n",
    "        metrics = fairness['metric'].unique()\n",
    "    ratios = sorted(fairness[ratio_type].unique())\n",
    "    other_ratio = 'gr' if ratio_type == 'ir' else 'ir'\n",
    "\n",
    "    fig, ax = plt.subplots(2, (len(metrics) - 1) // 2 + 1,\n",
    "                           sharex=True, sharey=True,\n",
    "                           figsize=(12, 9))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax[i % 2, i // 2].set_title(metric)\n",
    "        ax[i % 2, i // 2].set_ylabel('NaN probability')\n",
    "        ax[i % 2, i // 2].set_xlabel(ratio_type.upper())\n",
    "        ax[i % 2, i // 2].yaxis.set_major_formatter(PercentFormatter(1))\n",
    "        ax[i % 2, i // 2].spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "        for j, clf in enumerate(clfs):\n",
    "            subset = fairness[\n",
    "                (fairness['clf'] == clf) &\n",
    "                (fairness[other_ratio] == .5) &\n",
    "                (fairness['metric'] == metric)\n",
    "                ]\n",
    "            counts = subset.groupby(ratio_type)['value'].apply(lambda x: x.isna().sum() / x.shape[0])\n",
    "            ax[i % 2, i // 2].plot(ratios, counts,\n",
    "                                   label=clf, color=colours[j], marker='o', alpha=.6)\n",
    "\n",
    "    if ylim:\n",
    "        ax[0, 0].set_ylim(*ylim)\n",
    "    else:\n",
    "        ax[0, 0].set_ylim(0, ax[0, 0].get_ylim()[1] * 1.1)\n",
    "    ax[0, 0].set_xlim(0, 1)\n",
    "    ax[0, 0].legend(loc=0)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:29:29.392253Z",
     "start_time": "2024-02-07T00:29:28.214092Z"
    }
   },
   "outputs": [],
   "source": [
    "for ratio_type in ['ir', 'gr']:\n",
    "    fig = plot_nan(fairness_results_cv, ratio_type,\n",
    "                   metrics=[\n",
    "                       'Accuracy Equality Difference',\n",
    "                       'Statistical Parity Difference',\n",
    "                       'Equal Opportunity Difference',\n",
    "                       'Predictive Equality Difference',\n",
    "                       'Positive Predictive Parity Difference',\n",
    "                       'Negative Predictive Parity Difference',\n",
    "                   ])\n",
    "    fig.savefig(os.path.join(plots_dir, f'fairness_nan_{ratio_type}.png'))\n",
    "    fig.savefig(os.path.join(plots_dir, f'fairness_nan_{ratio_type}.pdf'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:29:29.402492Z",
     "start_time": "2024-02-07T00:29:29.397752Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_line_all(fairness: pd.DataFrame, metrics: list[str], ratio_type: str, fill='std', ylim=(-.5, .5)):\n",
    "    fig, axs = plt.subplots(\n",
    "        (len(metrics) - 1) // 2 + 1, 2,\n",
    "        sharex=True, sharey=True,\n",
    "        figsize=(12, 12)\n",
    "    )\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axs[i // 2, i % 2].set_ylabel(metric.replace('Difference', ''))\n",
    "\n",
    "        metrics = fairness['metric'].unique()\n",
    "        clfs = fairness['clf'].unique()\n",
    "        ratios = sorted(fairness[ratio_type].unique())\n",
    "        other_ratio = 'gr' if ratio_type == 'ir' else 'ir'\n",
    "        mean, stdev, err = {}, {}, {}\n",
    "\n",
    "        for r in ratios:\n",
    "            for clf in clfs:\n",
    "                subset = fairness[\n",
    "                    (fairness[ratio_type] == r) &\n",
    "                    (fairness['clf'] == clf) &\n",
    "                    (fairness[other_ratio] == .5) &\n",
    "                    (fairness['metric'] == metric) &\n",
    "                    fairness['value'].notna()\n",
    "                    ]\n",
    "                mean[(r, clf)] = subset['value'].mean(skipna=True)\n",
    "                stdev[(r, clf)] = subset['value'].std(skipna=True)\n",
    "                err[(r, clf)] = scipy.stats.sem(subset['value'], nan_policy='omit')\n",
    "\n",
    "        axs[i // 2, i % 2].axhline(0, color='black', linestyle='--', alpha=.9, lw=1)\n",
    "\n",
    "        for j, clf in enumerate(clfs):\n",
    "            axs[i // 2, i % 2].plot(ratios, [mean[(r, clf)] for r in ratios], label=clf, color=colours[j], marker='o', lw=1, alpha=.85)\n",
    "            if fill == 'err':\n",
    "                axs[i // 2, i % 2].fill_between(ratios,\n",
    "                                [mean[(r, clf)] - err[(r, clf)] for r in ratios],\n",
    "                                [mean[(r, clf)] + err[(r, clf)] for r in ratios],\n",
    "                                alpha=.15, color=colours[j])\n",
    "            elif fill == 'std':\n",
    "                axs[i // 2, i % 2].fill_between(ratios,\n",
    "                                [mean[(r, clf)] - stdev[(r, clf)] for r in ratios],\n",
    "                                [mean[(r, clf)] + stdev[(r, clf)] for r in ratios],\n",
    "                                alpha=.15, color=colours[j])\n",
    "\n",
    "        ratios_ticks = ['0.01\\n', '0.02',\n",
    "                        '0.05', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '0.95',\n",
    "                        '0.98', '\\n0.99']\n",
    "\n",
    "        axs[i // 2, i % 2].spines[['top', 'right']].set_visible(False)\n",
    "        axs[i // 2, i % 2].set_xticks(ratios, ratios_ticks, rotation=90)\n",
    "        axs[i // 2, i % 2].set_xlim(0, 1)\n",
    "        if i // 2 == 2:\n",
    "            axs[i // 2, i % 2].set_xlabel(x_labels[ratio_type])\n",
    "        if ylim:\n",
    "            axs[i // 2, i % 2].set_ylim(*ylim)\n",
    "\n",
    "    axs[0, 0].legend(loc=1,\n",
    "                     ncols=3)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:29:34.580977Z",
     "start_time": "2024-02-07T00:29:29.408557Z"
    }
   },
   "outputs": [],
   "source": [
    "timer.start()\n",
    "\n",
    "for ratio_type in ['ir', 'gr']:\n",
    "    fig = plot_line_all(fairness_results_cv, [\n",
    "        'Accuracy Equality Difference',\n",
    "        'Statistical Parity Difference',\n",
    "        'Equal Opportunity Difference',\n",
    "        'Predictive Equality Difference',\n",
    "        'Positive Predictive Parity Difference',\n",
    "        'Negative Predictive Parity Difference',\n",
    "    ], ratio_type, fill='std', ylim=(-.9, .9))\n",
    "    fig.savefig(os.path.join(plots_dir, f'fairness_all_{ratio_type}.png'))\n",
    "    fig.savefig(os.path.join(plots_dir, f'fairness_all_{ratio_type}.pdf'))\n",
    "    plt.close()\n",
    "    timer.checkpoint(f\"plotting for {ratio_type}\")\n",
    "timer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table with classification metrics\n",
    "\n",
    "this code directly prints the tables with formatting for LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:42.506004Z",
     "start_time": "2024-02-05T22:51:42.156518Z"
    }
   },
   "outputs": [],
   "source": [
    "clfs = results_cv['clf'].unique()\n",
    "scores = results_cv['metric'].unique()\n",
    "scores_strs = ['ROC AUC', 'G mean', 'recall', 'F1']\n",
    "\n",
    "\n",
    "for m, metric in enumerate(scores):\n",
    "    print(f'\\\\begin{{tabular}}{{{\"l l | \" + \"c \" * len(clfs)}}}')\n",
    "    print('\\\\multicolumn{' + str(len(scores) + 2) + '}{c}{' + scores_strs[m] + '} \\\\\\\\')\n",
    "    print('IR & GR & ' + ' & '.join(clfs) + ' \\\\\\\\')\n",
    "    for ratio_type, other_ratio in [['ir', 'gr'], ['gr', 'ir']]:\n",
    "        for ratio_val in sorted(results_cv[ratio_type].unique()):\n",
    "            subset = results_cv[\n",
    "                (results_cv['metric'] == metric) &\n",
    "                (results_cv[ratio_type] == ratio_val) &\n",
    "                (results_cv[other_ratio] == .5)\n",
    "                ]\n",
    "            if ratio_type == 'ir':\n",
    "                print(f'{ratio_val:.2f} & 0.50 ', end='')\n",
    "            else:\n",
    "                print(f'0.50 & {ratio_val:.2f} ', end='')\n",
    "\n",
    "            for clf in clfs:\n",
    "                print(f'& {subset[subset[\"clf\"] == clf][\"value\"].mean():.3f} ({subset[subset[\"clf\"] == clf][\"value\"].std():.3f}) ', end='')\n",
    "\n",
    "            print('\\\\\\\\')\n",
    "    print('\\\\end{tabular}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:42.514394Z",
     "start_time": "2024-02-05T22:51:42.482858Z"
    }
   },
   "outputs": [],
   "source": [
    "# timer.print()\n",
    "timer.to_file(fn='case_study.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:42.812784Z",
     "start_time": "2024-02-05T22:51:42.494914Z"
    }
   },
   "outputs": [],
   "source": [
    "# results to csv\n",
    "\n",
    "clfs = results_cv['clf'].unique()\n",
    "scores = results_cv['metric'].unique()\n",
    "scores_strs = ['ROC AUC', 'G mean', 'recall', 'F1']\n",
    "\n",
    "\n",
    "for m, metric in enumerate(scores):\n",
    "    lines = ['IR, GR, ' + ', '.join(clfs)]\n",
    "\n",
    "    ## print the results with formatting for LaTeX tables\n",
    "    # print(f'\\\\begin{{tabular}}{{{\"l l | \" + \"c \" * len(clfs)}}}')\n",
    "    # print('\\\\multicolumn{' + str(len(scores) + 2) + '}{c}{' + scores_strs[m] + '} \\\\\\\\')\n",
    "    # print('IR & GR & ' + ' & '.join(clfs) + ' \\\\\\\\')\n",
    "\n",
    "    for ratio_type, other_ratio in [['ir', 'gr'], ['gr', 'ir']]:\n",
    "        for ratio_val in sorted(results_cv[ratio_type].unique()):\n",
    "            subset = results_cv[\n",
    "                (results_cv['metric'] == metric) &\n",
    "                (results_cv[ratio_type] == ratio_val) &\n",
    "                (results_cv[other_ratio] == .5)\n",
    "                ]\n",
    "            if ratio_type == 'ir':\n",
    "                l = f'{ratio_val:.2f}, 0.50'\n",
    "            else:\n",
    "                l = f'0.50, {ratio_val:.2f}'\n",
    "            for clf in clfs:\n",
    "                l += f'  ,{subset[subset[\"clf\"] == clf][\"value\"].mean():.3f} ({subset[subset[\"clf\"] == clf][\"value\"].std():.3f})'\n",
    "            lines.append(l)\n",
    "\n",
    "    with open(os.path.join('out', f'clf_results_agg_{metric}.csv'), 'w') as f:\n",
    "        f.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T22:51:43.378506Z",
     "start_time": "2024-02-05T22:51:42.815149Z"
    }
   },
   "outputs": [],
   "source": [
    "# fairness results to csv\n",
    "\n",
    "clfs = fairness_results_cv['clf'].unique()\n",
    "scores = fairness_results_cv['metric'].unique()\n",
    "scores_strs = ['Accuracy Equality Difference', 'Statistical Parity Difference', 'Equal Opportunity Difference',\n",
    "               'Predictive Equality Difference', 'Positive Predictive Parity Difference', 'Negative Predictive Parity Difference']\n",
    "\n",
    "for m, metric in enumerate(scores):\n",
    "    lines = ['IR, GR, ' + ', '.join(clfs)]\n",
    "\n",
    "    for ratio_type, other_ratio in [['ir', 'gr'], ['gr', 'ir']]:\n",
    "        for ratio_val in sorted(fairness_results_cv[ratio_type].unique()):\n",
    "            subset = fairness_results_cv[\n",
    "                (fairness_results_cv['metric'] == metric) &\n",
    "                (fairness_results_cv[ratio_type] == ratio_val) &\n",
    "                (fairness_results_cv[other_ratio] == .5)\n",
    "                ]\n",
    "            if ratio_type == 'ir':\n",
    "                l = f'{ratio_val:.2f}, 0.50'\n",
    "            else:\n",
    "                l = f'0.50, {ratio_val:.2f}'\n",
    "            for clf in clfs:\n",
    "                l += f'  ,{subset[subset[\"clf\"] == clf][\"value\"].mean():.3f} ({subset[subset[\"clf\"] == clf][\"value\"].std():.3f})'\n",
    "            lines.append(l)\n",
    "\n",
    "    with open(os.path.join('out', f'clf_fairness_agg_{metric}.csv'), 'w') as f:\n",
    "        f.write('\\n'.join(lines))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

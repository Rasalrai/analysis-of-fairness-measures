{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Probability of Perfect Fairness\n",
    "\n",
    "calculations for different metrics, group ratios and imbalance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:43:17.351908Z",
     "end_time": "2023-04-26T20:43:17.353978Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare data from all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:43:17.395790Z",
     "end_time": "2023-04-26T20:43:17.396068Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 56\n",
    "# setting epsilon to another (small positive) value allows to calculate the probability of being epsilon-close to perfect fairness\n",
    "epsilon = 0\n",
    "\n",
    "calculations_dir = path.join('out', 'calculations', f'n{sample_size}')\n",
    "os.makedirs(calculations_dir, exist_ok=True)\n",
    "dataset_path = path.join('..', 'fairness-data-generator', 'out', f'Set(08,{sample_size}).bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculate values for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:43:19.095780Z",
     "end_time": "2023-04-26T20:43:19.105516Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_metrics = {    # { file: metric name }\n",
    "    'pos_pred_parity_diff.bin': 'Positive predictive parity difference',\n",
    "    'acc_equality_diff.bin': 'Accuracy equality difference',\n",
    "    'stat_parity.bin': 'Statistical parity difference',\n",
    "    'equal_opp_diff.bin': 'Equal opportunity difference',\n",
    "    'neg_pred_parity_diff.bin': 'Negative predictive parity difference',\n",
    "    'pred_equality_diff.bin': 'Predictive equality difference',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:43:19.468857Z",
     "end_time": "2023-04-26T20:43:19.470582Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ppf_diff(df, metrics, ratio_type, epsilon=0):\n",
    "    pf_probs, nan_probs = {}, {}\n",
    "\n",
    "    if epsilon == 0:\n",
    "        compute_diff_prob = lambda df: np.sum(df['diff'] == 0) / len(df)\n",
    "    else:\n",
    "        compute_diff_prob = lambda df: np.sum(np.abs(df['diff']) < epsilon) / len(df)\n",
    "\n",
    "    for metric_file, metric_name in metrics.items():\n",
    "        print(metric_name)\n",
    "        with open(path.join(calculations_dir,  metric_file), 'rb') as f:\n",
    "            diff_metric = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['diff'])\n",
    "        df = pd.concat([df, diff_metric], axis=1)\n",
    "\n",
    "        pf_bygroup = list()\n",
    "        nans_bygroup = list()\n",
    "\n",
    "        for gn, group in df.groupby(ratio_type):\n",
    "            if group['diff'].isna().all():\n",
    "                pf_bygroup.append([gn, np.nan])\n",
    "            else:\n",
    "                pf_bygroup.append([gn, compute_diff_prob(group)])\n",
    "            nans_bygroup.append([gn, group['diff'].isna().sum() / group.shape[0]])\n",
    "\n",
    "        pf_bygroup = pd.DataFrame(pf_bygroup, columns=[ratio_type, 'diff'])\n",
    "        pf_probs[metric_name] = pf_bygroup['diff']\n",
    "\n",
    "        nans_bygroup = pd.DataFrame(nans_bygroup, columns=[ratio_type, 'diff'])\n",
    "        nan_probs[metric_name] = nans_bygroup['diff']\n",
    "\n",
    "        # the dataframe (first col) can be reused for the next metric\n",
    "        df.drop('diff', axis=1, inplace=True)\n",
    "\n",
    "    pf_probs[ratio_type] = pf_bygroup[ratio_type]\n",
    "    pf_df = pd.DataFrame(pf_probs).reset_index()\n",
    "    pf_df.to_csv(path.join(calculations_dir, f'perfect_fairness_{ratio_type}_eps{epsilon}.csv'), index=False)\n",
    "\n",
    "    nan_probs[ratio_type] = nans_bygroup[ratio_type]\n",
    "    nan_df = pd.DataFrame(nan_probs).reset_index()\n",
    "    nan_df.to_csv(path.join(calculations_dir, f'nans_{ratio_type}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:43:20.797416Z",
     "end_time": "2023-04-26T20:43:22.107175Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ratio in ['ir', 'gr']:\n",
    "    with open(path.join(calculations_dir, f'{ratio}.bin'), 'rb') as f:\n",
    "        df = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=[ratio])\n",
    "    calculate_ppf_diff(df, diff_metrics, ratio, epsilon)\n",
    "del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

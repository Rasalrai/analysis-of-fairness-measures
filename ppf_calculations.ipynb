{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Probability of Perfect Fairness\n",
    "\n",
    "calculations for different metrics, group ratios and imbalance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare data from all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_cols = [\n",
    "    'i_tp',     # minority true positive\n",
    "    'i_fp',     # minority false positive\n",
    "    'i_tn',     # minority true negative\n",
    "    'i_fn',     # minority false negative\n",
    "    'j_tp',     # majority true positive\n",
    "    'j_fp',     # majority false positive\n",
    "    'j_tn',     # majority true negative\n",
    "    'j_fn',     # majority false negative\n",
    "]\n",
    "sample_size = 56    # 24\n",
    "epsilon = 1./50.\n",
    "\n",
    "calculations_dir = path.join('out', 'calculations', f'n{sample_size}')\n",
    "os.makedirs(calculations_dir, exist_ok=True)\n",
    "dataset_path = path.join('..', 'fairness-data-generator', 'out', f'Set(08,{sample_size}).bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculate values for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_metrics = {    # { file: metric name }\n",
    "    'acc_equality_diff.bin': 'Accuracy equality difference',\n",
    "    'equal_opp_diff.bin': 'Equal opportunity difference',\n",
    "    'neg_pred_parity_diff.bin': 'Negative predictive parity difference',\n",
    "    'pos_pred_parity_diff.bin': 'Positive predictive parity difference',\n",
    "    'pred_equality_diff.bin': 'Predictive equality difference',\n",
    "    'stat_parity.bin': 'Statistical parity'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_ppf_diff(df, metrics, ratio_type, epsilon=0):\n",
    "    diff_probs = {}\n",
    "\n",
    "    if epsilon == 0:\n",
    "        compute_diff_prob = lambda df: np.sum(df['diff'] == 0) / len(df)\n",
    "    else:\n",
    "        compute_diff_prob = lambda df: np.sum(np.abs(df['diff']) < epsilon) / len(df)\n",
    "\n",
    "    for metric_file, metric_name in metrics.items():\n",
    "        with open(path.join(calculations_dir,  metric_file), 'rb') as f:\n",
    "            diff_metric = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['diff'])\n",
    "        df = pd.concat([df, diff_metric], axis=1)\n",
    "\n",
    "        diff = df.groupby(ratio_type).apply(compute_diff_prob)\n",
    "        diff_probs[metric_name] = diff\n",
    "\n",
    "        df.drop('diff', axis=1, inplace=True)\n",
    "\n",
    "    result = pd.DataFrame(diff_probs)\n",
    "    result.reset_index(inplace=True)\n",
    "    result.to_csv(path.join(calculations_dir, f'ppf_{ratio_type}_eps{epsilon}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 22s, sys: 3min 46s, total: 11min 8s\n",
      "Wall time: 11min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ratio in ['gr', 'ir']:\n",
    "    with open(path.join(calculations_dir, f'{ratio}.bin'), 'rb') as f:\n",
    "        df = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=[ratio])\n",
    "    calculate_ppf_diff(df, diff_metrics, ratio, epsilon)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ratio metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_metrics = {    # { file: metric name }\n",
    "    'acc_equality_ratio.bin': 'Accuracy equality ratio',\n",
    "    'disp_impact.bin': 'Disparate impact',\n",
    "    'equal_opp_ratio.bin': 'Equal opportunity ratio',\n",
    "    'neg_pred_parity_ratio.bin': 'Negative predictive parity ratio',\n",
    "    'pos_pred_parity_ratio.bin': 'Positive predictive parity ratio',\n",
    "    # 'pred_parity_ratio.bin': 'Positive predictive parity ratio',\n",
    "    'pred_equality_ratio.bin': 'Predictive equality ratio',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_ppf_ratio(df, metrics, ratio_type, epsilon=0):\n",
    "    probs = {}\n",
    "    if epsilon == 0:\n",
    "        compute_prob = lambda df: np.sum(df['r'] == 1) / len(df)\n",
    "    else:\n",
    "        compute_prob = lambda df: np.sum(np.abs(df['r'] - 1) < epsilon) / len(df)\n",
    "\n",
    "    for metric_file, metric_name in metrics.items():\n",
    "        with open(path.join(calculations_dir,  metric_file), 'rb') as f:\n",
    "            metric_values = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=['r'])\n",
    "        df = pd.concat([df, metric_values], axis=1)\n",
    "\n",
    "        metric_probs = df.groupby(ratio_type).apply(compute_prob)\n",
    "        probs[metric_name] = metric_probs\n",
    "\n",
    "        df.drop('r', axis=1, inplace=True)\n",
    "\n",
    "    result = pd.DataFrame(probs)\n",
    "    result.reset_index(inplace=True)\n",
    "    result.to_csv(path.join(calculations_dir, f'ppf_{ratio_type}_ratios_eps{epsilon}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 3s, sys: 9min 5s, total: 28min 8s\n",
      "Wall time: 29min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epsilon in [0, .01, .02]:\n",
    "    for ratio in ['gr', 'ir']:\n",
    "        with open(path.join(calculations_dir, f'{ratio}.bin'), 'rb') as f:\n",
    "            df = pd.DataFrame(np.fromfile(f).astype(np.float16), columns=[ratio])\n",
    "        calculate_ppf_ratio(df, ratio_metrics, ratio, epsilon)\n",
    "        del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
